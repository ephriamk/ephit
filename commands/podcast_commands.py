import time
import asyncio
from pathlib import Path
from typing import Optional

from loguru import logger
from pydantic import BaseModel
from surreal_commands import CommandInput, CommandOutput, command

from open_notebook.config import DATA_FOLDER
from open_notebook.database.repository import ensure_record_id, repo_query
from open_notebook.domain.podcast import EpisodeProfile, PodcastEpisode, SpeakerProfile
from open_notebook.storage import (
    S3StorageError,
    build_episode_asset_key,
    is_s3_configured,
    upload_file,
)

try:
    from podcast_creator import configure, create_podcast
except ImportError as e:
    logger.error(f"Failed to import podcast_creator: {e}")
    raise ValueError("podcast_creator library not available")


def full_model_dump(model):
    if isinstance(model, BaseModel):
        return model.model_dump()
    elif isinstance(model, dict):
        return {k: full_model_dump(v) for k, v in model.items()}
    elif isinstance(model, list):
        return [full_model_dump(item) for item in model]
    else:
        return model


class PodcastGenerationInput(CommandInput):
    episode_profile: str
    speaker_profile: str
    episode_name: str
    content: str
    briefing_suffix: Optional[str] = None
    user_id: Optional[str] = None


class PodcastGenerationOutput(CommandOutput):
    success: bool
    episode_id: Optional[str] = None
    audio_file_path: Optional[str] = None
    transcript: Optional[dict] = None
    outline: Optional[dict] = None
    processing_time: float
    error_message: Optional[str] = None


@command("generate_podcast", app="open_notebook")
async def generate_podcast_command(
    input_data: PodcastGenerationInput,
) -> PodcastGenerationOutput:
    """
    Real podcast generation using podcast-creator library with Episode Profiles
    """
    start_time = time.time()

    try:
        logger.info(
            f"Starting podcast generation for episode: {input_data.episode_name}"
        )
        logger.info(f"Using episode profile: {input_data.episode_profile}")

        # 1. Load Episode and Speaker profiles from SurrealDB
        episode_profile = await EpisodeProfile.get_by_name(input_data.episode_profile)
        if not episode_profile:
            raise ValueError(
                f"Episode profile '{input_data.episode_profile}' not found"
            )

        speaker_profile = await SpeakerProfile.get_by_name(
            episode_profile.speaker_config
        )
        if not speaker_profile:
            raise ValueError(
                f"Speaker profile '{episode_profile.speaker_config}' not found"
            )

        logger.info(f"Loaded episode profile: {episode_profile.name}")
        logger.info(f"Loaded speaker profile: {speaker_profile.name}")

        # Import repo_query at module level to avoid scoping issues
        from open_notebook.database.repository import repo_query, ensure_record_id

        # 3. Load all profiles and configure podcast-creator
        episode_profiles = await repo_query("SELECT * FROM episode_profile")
        speaker_profiles = await repo_query("SELECT * FROM speaker_profile")

        # Transform the surrealdb array into a dictionary for podcast-creator
        episode_profiles_dict = {
            profile["name"]: profile for profile in episode_profiles
        }
        speaker_profiles_dict = {
            profile["name"]: profile for profile in speaker_profiles
        }

        # 4. Generate briefing
        briefing = episode_profile.default_briefing
        if input_data.briefing_suffix:
            briefing += f"\n\nAdditional instructions: {input_data.briefing_suffix}"

        # Find existing episode by name and user (created before command submission)
        # The episode should already exist from podcast_service.py
        episode = None
        if input_data.user_id:
            try:
                user_record_id = ensure_record_id(input_data.user_id)
                logger.info(f"Looking for episode: name='{input_data.episode_name}', owner={user_record_id}")
                results = await repo_query(
                    "SELECT * FROM episode WHERE name = $name AND owner = $owner ORDER BY created DESC LIMIT 1",
                    {"name": input_data.episode_name, "owner": user_record_id}
                )
                if results:
                    episode = PodcastEpisode(**results[0])
                    logger.info(f"‚úÖ Found existing episode: {episode.id} (name='{episode.name}')")
                    # Update episode with command reference if not already set
                    if input_data.execution_context and not episode.command:
                        episode.command = ensure_record_id(input_data.execution_context.command_id)
                        await episode.save()
                        logger.info(f"Updated episode {episode.id} with command reference")
                else:
                    logger.warning(f"‚ùå No episode found with name='{input_data.episode_name}' and owner={user_record_id}")
            except Exception as e:
                logger.error(f"‚ùå Failed to find existing episode: {e}")
                import traceback
                logger.error(traceback.format_exc())

        # Fallback: Create episode if not found (shouldn't happen, but for backward compatibility)
        if not episode:
            logger.warning(f"‚ö†Ô∏è Episode not found, creating new one (this shouldn't happen)")
            episode = PodcastEpisode(
                name=input_data.episode_name,
                episode_profile=full_model_dump(episode_profile.model_dump()),
                speaker_profile=full_model_dump(speaker_profile.model_dump()),
                command=ensure_record_id(input_data.execution_context.command_id)
                if input_data.execution_context
                else None,
                briefing=briefing,
                content=input_data.content,
                audio_file=None,
                transcript=None,
                outline=None,
                owner=ensure_record_id(input_data.user_id) if input_data.user_id else None,
            )
            await episode.save()
            logger.info(f"Created fallback episode: {episode.id}")

        configure("speakers_config", {"profiles": speaker_profiles_dict})
        configure("episode_config", {"profiles": episode_profiles_dict})

        logger.info("Configured podcast-creator with episode and speaker profiles")

        logger.info(f"Generated briefing (length: {len(briefing)} chars)")

        # 5. Create output directory
        output_dir = Path(f"{DATA_FOLDER}/podcasts/episodes/{input_data.episode_name}")
        output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Created output directory: {output_dir}")

        # 6. Generate podcast using podcast-creator
        logger.info("Starting podcast generation with podcast-creator...")

        # Load user's API keys from provider secrets
        from open_notebook.utils.provider_env import user_provider_context
        
        async with user_provider_context(input_data.user_id):
            result = await create_podcast(
                content=input_data.content,
                briefing=briefing,
                episode_name=input_data.episode_name,
                output_dir=str(output_dir),
                speaker_config=speaker_profile.name,
                episode_profile=episode_profile.name,
            )

        # The final audio file is at a predictable location: output_dir/audio/episode_name.mp3
        final_audio_path = output_dir / "audio" / f"{input_data.episode_name}.mp3"

        storage_url: Optional[str] = None

        if final_audio_path.exists():
            # DEFAULT: Store on Render's persistent disk (/mydata/podcasts/)
            # S3 is optional - only used if explicitly configured via environment variables
            if is_s3_configured():
                # Optional S3 path: Upload to S3, then delete local copy to save disk space
                key = build_episode_asset_key(
                    input_data.user_id,
                    str(episode.id),
                    final_audio_path.name,
                )
                try:
                    storage_url = await upload_file(
                        final_audio_path,
                        key,
                        content_type="audio/mpeg",
                    )
                    episode.audio_file = storage_url
                    logger.info(f"Uploaded podcast audio to S3: {storage_url}")
                    # Remove local copy after upload to save disk space
                    try:
                        await asyncio.to_thread(final_audio_path.unlink)
                    except OSError as unlink_error:
                        logger.warning(f"Failed to remove local audio file {final_audio_path}: {unlink_error}")
                except S3StorageError as exc:
                    # Fallback to persistent disk if S3 upload fails
                    logger.error(f"S3 upload failed, falling back to persistent disk storage: {exc}")
                    episode.audio_file = str(final_audio_path.resolve())
                    logger.info(f"Saved audio file to persistent disk: {episode.audio_file}")
            else:
                # DEFAULT PATH: Save to Render's persistent disk (/mydata/podcasts/)
                # This is the default behavior - no S3 configuration needed
                episode.audio_file = str(final_audio_path.resolve())
                logger.info(f"Saved audio file to persistent disk (default): {episode.audio_file}")
        else:
            logger.warning(f"Audio file not found at expected location: {final_audio_path}")
            # Try to get it from result if available, or construct path anyway
            if result and result.get("final_output_file_path"):
                episode.audio_file = str(result.get("final_output_file_path"))
            else:
                # Save the expected path even if file doesn't exist yet
                episode.audio_file = str(final_audio_path.resolve())
        
        # Save transcript and outline from result if available
        if result:
            episode.transcript = {
                "transcript": full_model_dump(result.get("transcript")) if result.get("transcript") else None
            }
            episode.outline = full_model_dump(result.get("outline")) if result.get("outline") else None
        
        logger.info(f"üíæ Saving episode {episode.id} with audio_file={episode.audio_file}")
        await episode.save()
        logger.info(f"‚úÖ Episode saved successfully")

        processing_time = time.time() - start_time
        logger.info(
            f"üéâ Successfully generated podcast episode: {episode.id} in {processing_time:.2f}s (audio: {episode.audio_file})"
        )

        return PodcastGenerationOutput(
            success=True,
            episode_id=str(episode.id),
            audio_file_path=episode.audio_file,
            transcript=episode.transcript,
            outline=episode.outline,
            processing_time=processing_time,
        )

    except Exception as e:
        processing_time = time.time() - start_time
        logger.error(f"Podcast generation failed: {e}")
        logger.exception(e)

        # Check for specific GPT-5 extended thinking issue
        error_msg = str(e)
        if "Invalid json output" in error_msg or "Expecting value" in error_msg:
            # This often happens with GPT-5 models that use extended thinking (<think> tags)
            # and put all output inside thinking blocks
            error_msg += (
                "\n\nNOTE: This error commonly occurs with GPT-5 models that use extended thinking. "
                "The model may be putting all output inside <think> tags, leaving nothing to parse. "
                "Try using gpt-4o, gpt-4o-mini, or gpt-4-turbo instead in your episode profile."
            )

        return PodcastGenerationOutput(
            success=False, processing_time=processing_time, error_message=error_msg
        )
